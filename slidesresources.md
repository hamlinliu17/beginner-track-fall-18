## Slide links (updated weekly)

- 10.16 [Intro to ML](tinyurl.com/btrackslides1)
- 10.23 [Math and Coding Overview](tinyurl.com/btrackslides2)
- 10.30 [Linear Regression: Theory](tinyurl.com/btracksl3)
- 11.06 [Linear Regression: Code](https://tinyurl.com/btrackslides4)
- 11.13 [Logistic Regression: Theory](tinyurl.com/btrackslides5)
- 11.20 [Logistic Regression: Code](tinyurl.com/btrackslides6)
- 11.27 [Neural Network Introduction](tinyurl.com/btrackslides7)

## Resources

### Math
#### Gradient Descent
- [A great intro to gradient descent.](https://medium.com/@montjoile/an-introduction-to-gradient-descent-algorithm-34cf3cee752b) The links included are super helpful and are also linked below for your covenience. 
- [Google's machine learning crash course.](https://developers.google.com/machine-learning/crash-course/) From interactive examples to clear explanations, Google's ML course is a great resource to understand or recap material.
- [An example of linear regression using gradient descent.](https://colab.research.google.com/drive/1PPVP27QaY2HaQNW3Gy3HT8MssdqXsHbM) Helpful to see a real example of code implementing what we're trying to do. Week 4's code will be fairly similar.
- [An online 3-D grapher.](https://academo.org/demos/3d-surface-plotter/) Plot functions, then calculate the gradient at certain points and locate the corresponding points on the graph. Check that if you move in the direction of the gradient, you're moving in the direction of steepest ascent. Now negate the gradient. Check that moving in the direction of the negated gradient causes you to move in the direction of steepest **descent**.

#### Backpropagataion 
- [How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)
- 
